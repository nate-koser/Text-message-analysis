{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef384509",
   "metadata": {},
   "source": [
    "## Sentiment analysis of US and Singaporean text messages\n",
    "\n",
    "The World Happiness Report for 2021 lists the United States as the 19th happiest country in the world, while Singapore rings in at 32nd*. Let's try some sentiment analysis to see if this reported discrepancy is borne out! The link below leads to a report on a corpus of text messages from students of different nationalities that were attending the National University of Singapore. Are the messages of Singaporean students rated as \"positive\" more than those of US students? Let's take a look.\n",
    "\n",
    "*source: https://worldhappiness.report/ed/2021/\n",
    "\n",
    "*corpus study: https://link.springer.com/article/10.1007/s10579-012-9197-9\n",
    "\n",
    "Start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "#nltk.download('twitter_samples')               be sure you have these! \n",
    "#nltk.download('punkt')                         \n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import punkt, FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8732b62",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "To do this, we're going to train a Naive Bayes model on a set of 10,000 tweets that were split into equal groups - tweets with \"positive\" sentiment and tweets with \"negative\" sentiment*. Using a training set of 7000 tokens and a test set of 3000 tokens,\n",
    "we can further test the performance of the model on the text message data.\n",
    "    \n",
    "**Item 94, \"Twitter Samples\" from: https://www.nltk.org/nltk_data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cbb08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data for analysis!\n",
    "\n",
    "#get dataframe of text messages\n",
    "textdf = pd.read_csv(\"clean_nus_sms.csv\")\n",
    "\n",
    "#drop columns we won't need/that aren't important for this task\n",
    "textdf = textdf.drop(['id', 'Unnamed: 0'], axis =1)\n",
    "\n",
    "#filter rows to get only USA and Singapore data\n",
    "textdf = textdf.loc[(textdf['country'] == 'United States') | (textdf['country'] == 'Singapore') ]\n",
    "\n",
    "#make a dataframe for each country \n",
    "textdf_us = textdf.loc[(textdf['country'] == 'United States')]\n",
    "textdf_sing = textdf = textdf.loc[(textdf['country'] == 'Singapore') ]\n",
    "\n",
    "# make a list of messages for each country\n",
    "textdf_us['Message'].dropna(inplace=True)\n",
    "textdf_sing['Message'].dropna(inplace=True)\n",
    "\n",
    "us_tokens = textdf_us['Message'].tolist()\n",
    "us_tokens_string = ' '.join(us_tokens)\n",
    "us_cleaned = re.sub('\\W+', ' ', us_tokens_string)\n",
    "\n",
    "sing_tokens = textdf_sing['Message'].tolist()\n",
    "sing_tokens_string = ' '.join(sing_tokens)\n",
    "sing_cleaned = re.sub('\\W+', ' ', sing_tokens_string)\n",
    "\n",
    "#tokenize\n",
    "pos_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "us_tokenized = word_tokenize(us_cleaned)\n",
    "sing_tokenized = word_tokenize(sing_cleaned)\n",
    "\n",
    "\n",
    "#normalize and lemmatize the text messages\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "us_no_stop = [word for word in us_tokenized if not word.lower() in stop_words]\n",
    "sing_no_stop = [word for word in sing_tokenized if not word.lower() in stop_words]\n",
    "\n",
    "us_clean_token = [WordNetLemmatizer().lemmatize(token) for token in us_no_stop]\n",
    "sing_clean_token = [WordNetLemmatizer().lemmatize(token) for token in sing_no_stop]\n",
    "\n",
    "\n",
    "#define a function that handles stop words and lemmatization for tweets\n",
    "def remove_noise(tokens, stop_words= ()):\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)', '', token)\n",
    "        \n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "#normalize and lemmatize the tweets\n",
    "pos_clean_token = []\n",
    "neg_clean_token = []\n",
    "\n",
    "for tokens in pos_tokens:\n",
    "    pos_clean_token.append(remove_noise(tokens, stop_words))\n",
    "for tokens in neg_tokens:\n",
    "    neg_clean_token.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "\n",
    "#define a function to get all words in a list for checking word frequencies\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(pos_clean_token)\n",
    "all_neg_words = get_all_words(neg_clean_token)\n",
    "all_us_words = get_all_words(us_clean_token)\n",
    "all_sing_words = get_all_words(sing_clean_token)\n",
    "\n",
    "#convert tokens to a dictionary for classification tasks\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "#to do this, have to make the text messages into a list of\n",
    "#tokenized sentences, rather than a list of strings        \n",
    "us_split_tokens = []\n",
    "sing_split_tokens = []\n",
    "for string in us_tokens:\n",
    "    us_split_tokens.append(string.split())\n",
    "for string in sing_tokens:\n",
    "    sing_split_tokens.append(string.split())\n",
    "    \n",
    "      \n",
    "pos_model_tokens = get_tweets_for_model(pos_clean_token)\n",
    "neg_model_tokens = get_tweets_for_model(neg_clean_token)\n",
    "us_model_tokens = get_tweets_for_model(us_split_tokens)\n",
    "sing_model_tokens = get_tweets_for_model(sing_split_tokens)\n",
    "\n",
    "\n",
    "#prepare tweet data for training/testing\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                    for tweet_dict in pos_model_tokens]\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                    for tweet_dict in neg_model_tokens]\n",
    "\n",
    "#prepare text message data for testing\n",
    "us_dataset = [(tweet_dict)\n",
    "                    for tweet_dict in us_model_tokens]\n",
    "sing_dataset = [(tweet_dict)\n",
    "                    for tweet_dict in sing_model_tokens]\n",
    "\n",
    "#make one combined set of negative and positive tweets for training\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "#shuffle so all positive tweets aren't first\n",
    "random.shuffle(dataset) \n",
    "\n",
    "#set apart 7000 for training, 3000 for testing\n",
    "train_data = dataset[:7000]  \n",
    "test_data = dataset[7000:]\n",
    "\n",
    "#build the model!\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470f81b",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "The model is built! How accurate is it, and what features does it find most informative? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b2eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2053.2 : 1.0\n",
      "                      :) = True           Positi : Negati =    998.8 : 1.0\n",
      "                     sad = True           Negati : Positi =     25.0 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.6 : 1.0\n",
      "                follower = True           Positi : Negati =     22.5 : 1.0\n",
      "              appreciate = True           Positi : Negati =     18.6 : 1.0\n",
      "                    glad = True           Positi : Negati =     17.2 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.8 : 1.0\n",
      "                followed = True           Negati : Positi =     14.8 : 1.0\n",
      "               community = True           Positi : Negati =     14.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy is:', classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9132f",
   "metadata": {},
   "source": [
    "Not too bad! 99% accuracy, and it finds smiley faces to be the most indicative of positivity, while the word 'sad' is the biggest hint for negativity. Some strange artifacts from training on Twitter data, such as \"follower\" and \"followed\" - as well as whatever \"x15\" is - but hopefully it still does okay on the text messages. \n",
    "\n",
    "\n",
    "Let's see what these kids are talking about! What are the most frequent words in the sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69db2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore 10 most frequent words: \n",
      "\n",
      "[('Haha', 4244), ('u', 3940), ('haha', 1760), ('go', 1671), ('le', 1425), ('got', 1330), ('lol', 1119), ('Hahaha', 1101), ('Lol', 1077), ('time', 979)]\n",
      "\n",
      "US 10 most frequent words: \n",
      "\n",
      "[('u', 271), ('get', 256), ('know', 245), ('Hi', 183), ('Thanks', 177), ('like', 161), ('want', 154), ('time', 126), ('got', 124), ('Lol', 123)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_us = FreqDist(sing_clean_token)\n",
    "print('Singapore 10 most frequent words: \\n')\n",
    "print(freq_dist_us.most_common(10))\n",
    "\n",
    "freq_dist_sing = FreqDist(us_clean_token)\n",
    "print('\\nUS 10 most frequent words: \\n') \n",
    "print(freq_dist_sing.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf6cba",
   "metadata": {},
   "source": [
    "The Singaporeans come out of the gate in a fit of laughter! They might be hard\n",
    "to beat. The Americans are more focused on \"time\" and \"know\"-ing, which is \n",
    "not much fun! \n",
    "\n",
    "Let's see how the model performs on a couple of individual messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f089f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a) Clearly negative example: I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "Negative\n",
      "\n",
      "b) Clearly positive example: Have a good show!\n",
      "\n",
      "Positive\n",
      "\n",
      "c) Positive, but not straightforward: Heee?? I cant wait too sweerty. Muacks u much much\n",
      "\n",
      "Negative\n",
      "\n",
      "d) Neutral: Okay i'll be at canteen 2.\n",
      "\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print('\\na) Clearly negative example: ' +  us_tokens[65])\n",
    "print('\\n' + classifier.classify(dict([token, True] for token in us_dataset[65])))\n",
    "\n",
    "\n",
    "print('\\nb) Clearly positive example: ' + us_tokens[168])\n",
    "print('\\n' + classifier.classify(dict([token, True] for token in us_dataset[168])))\n",
    "\n",
    "print('\\nc) Positive, but not straightforward: ' + sing_tokens[625])\n",
    "print('\\n' + classifier.classify(dict([token, True] for token in sing_dataset[625])))\n",
    "\n",
    "print('\\nd) Neutral: ' + sing_tokens[8888])\n",
    "print('\\n' + classifier.classify(dict([token, True] for token in sing_dataset[8888])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d3636",
   "metadata": {},
   "source": [
    "It appears to do pretty well, though there are also certainly ways to refine it that I discuss a bit below. Let's try to get a sense of the overall positivity/negativity. Let's use a variable to tally the positive and negative count for each country, which we can use to make a \"happiness ratio\", called HR. Then we iterate through each list and add to the appropriate tally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5399a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_neg_tally = 0\n",
    "us_pos_tally = 0\n",
    "sing_neg_tally = 0\n",
    "sing_pos_tally = 0\n",
    "\n",
    "\n",
    "for sent in us_dataset:\n",
    "    if classifier.classify(dict([token, True] for token in sent)) == \"Positive\":\n",
    "        us_pos_tally += 1\n",
    "    else:\n",
    "        us_neg_tally += 1\n",
    "        \n",
    "        \n",
    "for sent in sing_dataset:\n",
    "    if classifier.classify(dict([token, True] for token in sent)) == \"Positive\":\n",
    "        sing_pos_tally += 1\n",
    "    else:\n",
    "        sing_neg_tally += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fb16a",
   "metadata": {},
   "source": [
    "Now, we divide the positive scores by the negative scores to get the happiness ratio! Since we divide by the negative tally, a higher HR means higher happiness. If HR < 1, this means there were more negative texts than positive ones. Let's see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da1f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The happiness ratio for Singaporean text messages is: 0.5832134637514385\n",
      "\n",
      "The happiness ratio for US text messages is: 0.6457418788410887\n"
     ]
    }
   ],
   "source": [
    "sing_happy_ratio = sing_pos_tally / sing_neg_tally\n",
    "print('The happiness ratio for Singaporean text messages is: ' + str(sing_happy_ratio))\n",
    "\n",
    "\n",
    "us_happy_ratio = us_pos_tally / us_neg_tally\n",
    "print('\\nThe happiness ratio for US text messages is: ' + str(us_happy_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c9f9f",
   "metadata": {},
   "source": [
    "It's pretty close! The US messages might just barely have an edge. This mirrors the World Happiness Rankings too, where the two countries were both relatively highly-ranked and not super far apart. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31688f72",
   "metadata": {},
   "source": [
    "### Areas for further consideration\n",
    "\n",
    "There are many ways the analysis could be extended/refined in the future. Here are just a few ideas:\n",
    "\n",
    "* *Normalize the happiness ratio*:\n",
    "\n",
    "    * The happiness ratio is not super informative in a vaccuum - maybe it could be normalized somehow? Put the tallies in a data frame and MinMax normalize to values between 0 (least happy) and 1 (most happy)? \n",
    "    \n",
    "* *Cultural context*:\n",
    "\n",
    "    * Lots of the slang terms in the texts escaped the stopword filter. Those could be added pretty simply, but it might affect the model's predictions if it keys in on these slang terms to make the positive/negative distinction. There are also lots of non-English terms in the Singaporean data that require cultural context. \"Le\" for example turns up a bunch, and is apparently\n",
    "a discourse marker denoting uncertainty. This kind of context is really important!\n",
    "\n",
    "* *Coding*:\n",
    "\n",
    "    * I ended up using slightly different code to process tweets and texts because of the different structure of each data set. Surely there must be a more parsimonious solution?\n",
    "    \n",
    "* *Bugs(?):*\n",
    "\n",
    "    * The evaluation of individual messages seems to change each time the code is run. Is this just the \n",
    "variation of the model? Are the examples that are not clearly negative or positive causing some flip-flopping?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
